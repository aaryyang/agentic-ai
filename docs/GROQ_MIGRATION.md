# Migration Guide: OpenAI to Groq (Llama)

## âœ… What Was Changed

### 1. **Environment Variables (.env)**
```bash
# OLD - OpenAI Configuration
OPENAI_API_KEY=sk-your-openai-key
OPENAI_MODEL=gpt-4

# NEW - Groq Configuration  
GROQ_API_KEY=your-groq-api-key-here
GROQ_MODEL=meta-llama/llama-4-scout-17b-16e-instruct
GROQ_STREAMING=false
```

### 2. **Dependencies (requirements.txt)**
```bash
# ADDED
groq==0.4.1

# MOVED TO OPTIONAL
# langchain-openai==0.1.8  # Now optional fallback
# openai==1.35.0          # Now optional fallback
```

### 3. **Core Agent (core/engine/core_agent.py)**
- Replaced `ChatOpenAI` with `Groq` client
- Updated message format conversion
- Added streaming support preparation
- Updated error handling for Groq API

### 4. **Configuration (config/settings.py)**
- Added Groq-specific settings
- Made OpenAI settings optional
- Added streaming configuration

## ðŸš€ How to Use

### 1. **Get Groq API Key**
1. Go to [Groq Console](https://console.groq.com)
2. Create account and get API key
3. Update `.env` file:
   ```bash
   GROQ_API_KEY=your_actual_groq_key_here
   ```

### 2. **Install Dependencies**
```bash
pip install -r requirements.txt
```

### 3. **Test the Integration**
```bash
python scripts/test_groq.py
```

### 4. **Start the System**
```bash
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

## ðŸŽ¯ Key Benefits

### **Performance**
- **Faster responses**: Groq's LPU architecture provides ultra-fast inference
- **Lower latency**: Optimized for real-time applications
- **Better streaming**: Native support for streaming responses

### **Cost Efficiency**
- **Lower costs**: More competitive pricing than OpenAI
- **Better value**: High-quality Llama models at lower cost

### **Model Quality**
- **Llama 4 Scout**: Latest Meta model with excellent performance
- **17B parameters**: Good balance of quality and speed
- **Instruct-tuned**: Optimized for conversational AI

## ðŸ”§ Advanced Configuration

### **Enable Streaming (Optional)**
```python
# In .env
GROQ_STREAMING=true

# This will enable real-time streaming responses
# Perfect for chat applications
```

### **Model Options**
You can change the model in `.env`:
```bash
# Other available models
GROQ_MODEL=llama-3.2-90b-text-preview
GROQ_MODEL=mixtral-8x7b-32768
GROQ_MODEL=gemma-7b-it
```

### **Temperature & Token Settings**
```bash
DEFAULT_AGENT_TEMPERATURE=0.7  # Creativity (0.0-2.0)
MAX_TOKENS=1000                # Max response length
```

## ðŸ§ª Testing

### **Verify Integration**
```bash
python scripts/test_groq.py
```

### **Test API Endpoints**
```bash
# Start server
uvicorn api.main:app --reload

# Test in browser
http://localhost:8000/docs

# Test chat endpoint
curl -X POST "http://localhost:8000/agent/chat" \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello", "user_id": "test"}'
```

## ðŸš¨ Troubleshooting

### **Common Issues**

1. **"groq module not found"**
   ```bash
   pip install groq==0.4.1
   ```

2. **API Key Issues**
   - Check if GROQ_API_KEY is set correctly
   - Verify key has proper permissions
   - System will run in mock mode if key is invalid

3. **Import Errors**
   ```bash
   # Reinstall dependencies
   pip install -r requirements.txt --force-reinstall
   ```

### **Fallback Mode**
If Groq API fails, the system automatically falls back to mock responses, ensuring your application stays operational.

## ðŸ“Š Performance Comparison

| Metric | OpenAI GPT-4 | Groq Llama 4 Scout |
|--------|-------------|-------------------|
| Response Time | ~2-5s | ~0.5-1s |
| Cost per 1K tokens | ~$0.03 | ~$0.01 |
| Streaming | Yes | Yes (optimized) |
| Quality | Excellent | Very Good |

## ðŸŽ‰ You're Ready!

Your AI Agent system is now powered by Groq's lightning-fast Llama model. Enjoy the improved performance and lower costs!
